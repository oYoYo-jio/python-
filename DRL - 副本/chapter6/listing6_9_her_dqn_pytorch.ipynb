{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network Variant Hindsight Replay DQN (HER-DQN) implementation (PyTorch).\n",
    "\n",
    "In this notebook, we will implement HER-DQN variant of DQN. We saw DQN in `6_1_dqn_pytorch.ipynb`. We will borrow some parts of it. However, in this notebook we will create a different environment which has no reward till you hit the target - an example of sparse reward setup. We will see how the approach of HER helps us make it efficient to learn in the sparse reward setup. \n",
    "\n",
    "### RECAP\n",
    "\n",
    "$$ \n",
    "\\DeclareMathOperator*{\\max}{max}$$\n",
    "\n",
    "Q Learning control is carried out by sampling step by step and updating Q values at each step. We use ε-greedy policy to explore and generate samples. However, the policy learnt is a deterministic greedy policy with no exploration. We can carryout updates online i.e. we take a step and use `(current state, action, reward and next_state)` tuple to update. \n",
    "\n",
    "In case of function approximation using neural network, the input to the network is the state and output is the q(s,a) for all the actions in the state `s`. It is denoted as $ \\hat{q}(s_t,a; w_{t}) $, where $w_{t}$ is the weights of the neural network which we are trying to learn as part of DQN learning. \n",
    "\n",
    "We use two networks, one target network to get the max q-value of next state denoted by $ \\max_a \\hat {q}(s_{t+1},a; w^{-}_{t}) $ and the primary network with weights $w_{t}$ which we are updated based on back propagation of the TD error through the network.\n",
    "\n",
    "The Update equation is given below. This is the online version:\n",
    "$$ w_{t+1} \\leftarrow w_t + \\alpha [ R_{t+1} + \\gamma . \\max_{a} \\hat{q}(S_{t+1},a,w^{-}_{t}) – \\hat{q}(S_t,A_t,w_t)] \\nabla \\hat{q}(S_t,A_t,w_t)$$\n",
    "\n",
    "Online update with neural network with millions of weights does not work well. Accordingly, We use experience replay (aka Replay Buffer).  We use a behavior policy to explore the environment and store the samples `(s, a, r, s', done)` in a buffer. The samples are generated using an exploratory behavior policy while we improve a deterministic target policy using q-values. \n",
    "\n",
    "Therefore, we can always use older samples from behavior policy and apply them again and again. We can keep the buffer size fixed to some pre-determined size and keep deleting the older samples as we collect new ones. This process makes learning sample efficient by reusing a sample multiple times and also removing temporal dependence of the samples we would otherwise see while following a trajectory.\n",
    "\n",
    "The update equation with batch update with minor modifications is given below. We collect samples of transitions (current state, action, reward, next state) in a buffer. Where each sample is denoted as a tuple: \n",
    "\n",
    "$$ (s_{i}, a_{i}, r_{i}, s^{'}_{i}, done_{i})$$\n",
    "\n",
    "Subscript (i) denotes ith sample. We take N samples from experience replay selecting randomly and update the weights. Subscript (t) denotes the index of weight updates. If the current state is done, as denoted by `done` flag, the target is just the reward as terminal states have zero value. The final update equation is as given below:\n",
    "\n",
    "$$w_{t+1} \\leftarrow w_t + \\alpha \\frac{1}{N} \\sum_{i=1}^{N} \\left[ r_i + \\left( (1-done_i) . \\gamma .  \\max_{a^{'}} \\hat{q}(s_{i}^{'},a^{'};w^{-}_{t}) \\right) – \\hat{q}(s_i,a_i;w_t) \\right] \\nabla \\hat{q}(s_i,a_i;w_t)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve, gaussian\n",
    "import math\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import time\n",
    "from tqdm import trange\n",
    "import glob\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "from IPython.display import HTML, clear_output\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay\n",
    "\n",
    "We will use the replay buffer we saw in chapter 4 listings. Replay buffer is very important in DQN to break the correlation between samples. We use a behavior policy (epsilon greedy) to sample from the environment and store the transitions (s,a,r,s',done) into a buffer. These samples are used multiple times in a learning making the process sample efficient. \n",
    "\n",
    "The interface to ReplayBuffer is:\n",
    "* `exp_replay.add(state, action, reward, next_state, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
    "* `exp_replay.sample(batch_size)` - returns states, actions, rewards, next_states and done_flags for `batch_size` random samples.\n",
    "* `len(exp_replay)` - returns number of elements stored in replay buffer.\n",
    "\n",
    "We have modified the implementation a bit to make it more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.size = size #max number of items in buffer\n",
    "        self.buffer =[] #array to holde buffer\n",
    "        self.next_id = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        item = (state, action, reward, next_state, done)\n",
    "        if len(self.buffer) < self.size:\n",
    "           self.buffer.append(item)\n",
    "        else:\n",
    "            self.buffer[self.next_id] = item\n",
    "        self.next_id = (self.next_id + 1) % self.size\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size)\n",
    "        samples = [self.buffer[i] for i in idxs]\n",
    "        states, actions, rewards, next_states, done_flags = list(zip(*samples))\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(done_flags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning with... Q-learning\n",
    "Here we write a function similar to tabular q-learning. We will calculate average TD error per batch using the equation: \n",
    "\n",
    "$$ L =  \\frac{1}{N} \\sum_{i=1}^{N} \\left[ r_i + \\left( (1-done_i) . \\gamma .  \\max_{a^{'}} \\hat{q}(s_{i}^{'},a^{'};w^{-}_{t}) \\right) – \\hat{q}(s_i,a_i;w_t) \\right]^2$$\n",
    "\n",
    "\n",
    "$$ \\nabla_w L =   - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ r_i + \\left( (1-done_i) . \\gamma .  \\max_{a^{'}} \\hat{q}(s_{i}^{'},a^{'};w^{-}_{t}) \\right) – \\hat{q}(s_i,a_i;w_t) \\right] \\nabla \\hat{q}(s_i,a_i;w_t)$$\n",
    "\n",
    "\n",
    "$\\hat{q}(s',A;w^{-})$ is calculated using target network whose weights are held constant and refreshed periodically from the agent learning network. \n",
    "\n",
    "Target is given by following:\n",
    "* non terminal state: $r_i +  \\gamma .  \\max_{a^{'}} \\hat{q}(s_{i}^{'},a^{'};w^{-}_{t})$\n",
    "* terminal state: $ r_i $\n",
    "\n",
    "We then carryout back propagation through the agent network to update the weights using equation below:\n",
    "\n",
    "\n",
    "$$ \n",
    "\\DeclareMathOperator*{\\max}{max} w_{t+1} \\leftarrow w_t - \\alpha \\nabla_{w}L$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_loss_dqn(agent, target_network, states, actions, rewards, next_states, done_flags,\n",
    "                    gamma=0.99, device=device):\n",
    "\n",
    "    # convert numpy array to torch tensors\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float)\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.long)\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    done_flags = torch.tensor(done_flags.astype('float32'),device=device,dtype=torch.float)\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    # use agent network\n",
    "    q_s = agent(states)\n",
    "\n",
    "    # select q-values for chosen actions\n",
    "    q_s_a = q_s[range(\n",
    "        len(actions)), actions]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # compute q-values for all actions in next states\n",
    "        # use target network\n",
    "        q_s1 = target_network(next_states)\n",
    "\n",
    "\n",
    "        # compute Qmax(next_states, actions) using predicted next q-values\n",
    "        q_s1_a1max,_ = torch.max(q_s1, dim=1)\n",
    "\n",
    "        # compute \"target q-values\" \n",
    "        target_q = rewards + gamma * q_s1_a1max * (1-done_flags)\n",
    "\n",
    "    # mean squared error loss to minimize\n",
    "    loss = torch.mean((q_s_a - target_q.detach()) ** 2)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hindsight Replay\n",
    "\n",
    "In the paper by OpenAI in 2018, https://arxiv.org/pdf/1707.01495.pdf, the authors presented a sample efficient approach to learn in the environment where the rewards are sparse and binary. The common approach is to shape the reward function in a way to guide the agents towards optimization. This is not generalizable. It requires a deep understanding of the domain to design a suitable reward function.\n",
    "\n",
    "Compared to RL agents, which learn from a successful outcome, humans seem to learn not just from that but also from unsuccessful outcomes. This is the basis of the idea proposed in Hindsight replay approach known as **HER**. While **HER** can be combined with various RL approaches. In the code cells below we will use HER with Dueling DQN giving us **HER-DQN**\n",
    "\n",
    "In HER approach, after an episode is played out which let us say was not successful, we form a secondary objective where the original goal is replaced with the last state before termination as a goal for this trajectory since this trajectory ended in that state. \n",
    "\n",
    "Say a episode has been played out $ s_0, s_1, .... s_T$. Normally we store in Replay buffer a tuple of $(s_t, a_t, r, s_{t+1}, done)$. Let us say the goal for this episode was $g$ which could not be achieved in this run. In HER approach we will store following to the replay buffer:\n",
    "\n",
    "* $(s_t||g, a_t, r, s_{t+1}|g, done)$\n",
    "* $(s_t||g', a_t, r(s_t, a_t, g'), s_{t+1}|g', done)$: other state transitions based on synthetic goals like last state of the episode as a sub-goal say g'. The reward is modified to show how state transition $s_t \\rightarrow s_{t+1}$, was good or bad for the sub-goal of $g'$. \n",
    "\n",
    "Original paper discusses various strategies for forming these subgoals. We will use one of them called `future`:\n",
    "* future — replay with k random states which come from the same episode as the transition being replayed and were observed after it.\n",
    "\n",
    "We also use a different kind of environment from our past notebooks. We will use an environment as used in the paper that of bit-flipping experiment. Say you have a vector with n-bits, each being binary in the range {0,1}. Therefore there are $2^n$ combinations possible. At reset, environment starts in a n-bit configuration randomly and the goal is also randomly picked to be some different n-bit configuration. Each action is to flip a bit. The bit to be flipped is the policy $\\pi(a|s)$ that agent is trying to learn. An episode ends if the agent is able to find the right configuration matching the goal or when agent has exhausted `n` actions in an episode.\n",
    "\n",
    "The authors show that a regular DQN, where the state (configuration of n-bits) is represented as a deep network, it is almost impossible for a regular DQN agent to learn beyond 15 digit combinations. However, coupled with HER-DQN approach, the agent is able to learn easily even for large digit combinations like 50 or so. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitFlipEnvironment:\n",
    "\n",
    "    def __init__(self, bits):\n",
    "        self.bits = bits\n",
    "        self.state = np.zeros((self.bits, ))\n",
    "        self.goal = np.zeros((self.bits, ))\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.random.randint(2, size=self.bits).astype(np.float32)\n",
    "        self.goal = np.random.randint(2, size=self.bits).astype(np.float32)\n",
    "        if np.allclose(self.state, self.goal):\n",
    "            self.reset()\n",
    "        return self.state.copy(), self.goal.copy()\n",
    "\n",
    "    def step(self, action):\n",
    "        self.state[action] = 1 - self.state[action]  # Flip the bit on position of the action\n",
    "        reward, done = self.compute_reward(self.state, self.goal)\n",
    "        return self.state.copy(), reward, done\n",
    "\n",
    "    def render(self):\n",
    "        print(\"State: {}\".format(self.state.tolist()))\n",
    "        print(\"Goal : {}\\n\".format(self.goal.tolist()))\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_reward(state, goal):\n",
    "        done = np.allclose(state, goal)\n",
    "        return 0.0 if done else -1.0, done\n",
    "\n",
    "# a simplified version of DuelingDQN with lesser number of layers\n",
    "class DuelingMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, n_actions, epsilon=1.0):\n",
    "        super().__init__()\n",
    "        self.state_size = state_size\n",
    "        self.n_actions = n_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.linear = nn.Linear(state_size, 256)\n",
    "        self.v = nn.Linear(256, 1)\n",
    "        self.adv = nn.Linear(256, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear(x))\n",
    "        v = self.v(x)\n",
    "        adv = self.adv(x)\n",
    "        qvalues = v + adv - adv.mean(dim=1, keepdim=True)\n",
    "        return qvalues\n",
    "    \n",
    "    def get_qvalues(self, states):\n",
    "        # input is an array of states in numpy and outout is Qvals as numpy array\n",
    "        states = torch.tensor(states, device=device, dtype=torch.float32)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        # sample actions from a batch of q_values using epsilon greedy policy\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape        \n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(-1)\n",
    "        should_explore = np.random.choice(\n",
    "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_her(env, agent, target_network, optimizer, td_loss_fn):\n",
    "\n",
    "    success_rate = 0.0\n",
    "    success_rates = []\n",
    "    \n",
    "    exp_replay = ReplayBuffer(10**6)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Decay epsilon linearly from eps_max to eps_min\n",
    "        eps = max(eps_max - epoch * (eps_max - eps_min) / int(num_epochs * exploration_fraction), eps_min)\n",
    "        print(\"Epoch: {}, exploration: {:.0f}%, success rate: {:.2f}\".format(epoch + 1, 100 * eps, success_rate))\n",
    "        agent.epsilon = eps\n",
    "        target_network.epsilon = eps\n",
    "\n",
    "        successes = 0\n",
    "        for cycle in range(num_cycles):\n",
    "\n",
    "            for episode in range(num_episodes):\n",
    "\n",
    "                # Run episode and cache trajectory\n",
    "                episode_trajectory = []\n",
    "                state, goal = env.reset()\n",
    "\n",
    "                for step in range(num_bits):\n",
    "\n",
    "                    state_ = np.concatenate((state, goal))\n",
    "                    qvalues = agent.get_qvalues([state_])\n",
    "                    action = agent.sample_actions(qvalues)[0]\n",
    "                    next_state, reward, done = env.step(action)\n",
    "                    \n",
    "                    episode_trajectory.append((state, action, reward, next_state, done))\n",
    "                    state = next_state\n",
    "                    if done:\n",
    "                        successes += 1\n",
    "                        break\n",
    "\n",
    "                # Fill up replay memory\n",
    "                steps_taken = step\n",
    "                for t in range(steps_taken):\n",
    "\n",
    "                    # Usual experience replay\n",
    "                    state, action, reward, next_state, done = episode_trajectory[t]\n",
    "                    state_, next_state_ = np.concatenate((state, goal)), np.concatenate((next_state, goal))\n",
    "                    exp_replay.add(state_, action, reward, next_state_, done)\n",
    "\n",
    "                    # Hindsight experience replay\n",
    "                    for _ in range(future_k):\n",
    "                        future = random.randint(t, steps_taken)  # index of future time step\n",
    "                        new_goal = episode_trajectory[future][3]  # take future next_state from (s,a,r,s',d) and set as goal\n",
    "                        new_reward, new_done = env.compute_reward(next_state, new_goal)\n",
    "                        state_, next_state_ = np.concatenate((state, new_goal)), np.concatenate((next_state, new_goal))\n",
    "                        exp_replay.add(state_, action, new_reward, next_state_, new_done)\n",
    "\n",
    "            # Optimize DQN\n",
    "            for opt_step in range(num_opt_steps):\n",
    "                # train by sampling batch_size of data from experience replay\n",
    "                states, actions, rewards, next_states, done_flags = exp_replay.sample(batch_size)\n",
    "                # loss = <compute TD loss>\n",
    "                optimizer.zero_grad()\n",
    "                loss = td_loss_fn(agent, target_network, \n",
    "                                  states, actions, rewards, next_states, done_flags,                  \n",
    "                                  gamma=0.99,\n",
    "                                  device=device)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "            target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "        success_rate = successes / (num_episodes * num_cycles)\n",
    "        success_rates.append(success_rate)\n",
    "\n",
    "    # print graph\n",
    "    plt.plot(success_rates, label=\"HER-DQN\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Success rate\")\n",
    "    plt.title(\"Number of bits: {}\".format(num_bits))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bits = 50 \n",
    "n_actions = num_bits\n",
    "state_size = 2*num_bits\n",
    "\n",
    "future_k = 4\n",
    "num_epochs = 40\n",
    "num_cycles = 50\n",
    "num_episodes = 16\n",
    "num_opt_steps = 40\n",
    "eps_max=0.2\n",
    "eps_min=0.0\n",
    "exploration_fraction=0.5\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "env = BitFlipEnvironment(num_bits)\n",
    "\n",
    "agent = DuelingMLP(state_size, n_actions, epsilon=1).to(device)\n",
    "target_network = DuelingMLP(state_size, n_actions, epsilon=1).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, exploration: 20%, success rate: 0.00\n",
      "Epoch: 2, exploration: 19%, success rate: 0.00\n",
      "Epoch: 3, exploration: 18%, success rate: 0.00\n",
      "Epoch: 4, exploration: 17%, success rate: 0.00\n",
      "Epoch: 5, exploration: 16%, success rate: 0.00\n",
      "Epoch: 6, exploration: 15%, success rate: 0.00\n",
      "Epoch: 7, exploration: 14%, success rate: 0.00\n",
      "Epoch: 8, exploration: 13%, success rate: 0.01\n",
      "Epoch: 9, exploration: 12%, success rate: 0.01\n",
      "Epoch: 10, exploration: 11%, success rate: 0.02\n",
      "Epoch: 11, exploration: 10%, success rate: 0.03\n",
      "Epoch: 12, exploration: 9%, success rate: 0.07\n",
      "Epoch: 13, exploration: 8%, success rate: 0.11\n",
      "Epoch: 14, exploration: 7%, success rate: 0.15\n",
      "Epoch: 15, exploration: 6%, success rate: 0.22\n",
      "Epoch: 16, exploration: 5%, success rate: 0.28\n",
      "Epoch: 17, exploration: 4%, success rate: 0.34\n",
      "Epoch: 18, exploration: 3%, success rate: 0.41\n",
      "Epoch: 19, exploration: 2%, success rate: 0.52\n",
      "Epoch: 20, exploration: 1%, success rate: 0.59\n",
      "Epoch: 21, exploration: 0%, success rate: 0.67\n",
      "Epoch: 22, exploration: 0%, success rate: 0.78\n",
      "Epoch: 23, exploration: 0%, success rate: 0.84\n",
      "Epoch: 24, exploration: 0%, success rate: 0.89\n",
      "Epoch: 25, exploration: 0%, success rate: 0.91\n",
      "Epoch: 26, exploration: 0%, success rate: 0.95\n",
      "Epoch: 27, exploration: 0%, success rate: 0.97\n",
      "Epoch: 28, exploration: 0%, success rate: 0.98\n",
      "Epoch: 29, exploration: 0%, success rate: 0.99\n",
      "Epoch: 30, exploration: 0%, success rate: 0.99\n",
      "Epoch: 31, exploration: 0%, success rate: 1.00\n",
      "Epoch: 32, exploration: 0%, success rate: 1.00\n",
      "Epoch: 33, exploration: 0%, success rate: 1.00\n",
      "Epoch: 34, exploration: 0%, success rate: 1.00\n",
      "Epoch: 35, exploration: 0%, success rate: 1.00\n",
      "Epoch: 36, exploration: 0%, success rate: 1.00\n",
      "Epoch: 37, exploration: 0%, success rate: 1.00\n",
      "Epoch: 38, exploration: 0%, success rate: 1.00\n",
      "Epoch: 39, exploration: 0%, success rate: 1.00\n",
      "Epoch: 40, exploration: 0%, success rate: 1.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_her(env, agent, target_network, optimizer, td_loss_fn=td_loss_dqn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this notebook we saw how to train a DQN agent with experience replay and target networks. We also saw HER variant which augmented replay buffer with additional sub goals to make it sample efficient **Hindsight Experience Replay (HER-DQN)**. While we combined HER with DQN, it could also be combined with various other learning algorithms including ones from POlicy gradient versions from later chapters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
